# -*- coding: utf-8 -*-
"""Predict_Air_Quality.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dKSsSSUKxAcHPh0VCj0PnaefhSvRU0C5

# Proyek: Air Quality Predictive Analytics

# Latar Belakang

Kualitas udara menjadi salah satu indikator penting dalam menjaga kesehatan masyarakat dan kelestarian lingkungan, terutama di daerah perkotaan yang padat penduduk dan memiliki tingkat polusi yang tinggi. Peningkatan aktivitas manusia, seperti transportasi, industri, dan pembakaran bahan bakar fosil, berkontribusi signifikan terhadap pencemaran udara. Hal ini menyebabkan peningkatan konsentrasi polutan seperti CO, PM10, PM2.5, O₃, dan NO₂, yang secara langsung memengaruhi kesehatan manusia dan lingkungan.

Beberapa isu utama yang melatarbelakangi pentingnya proyek ini:
1. Dampak Kesehatan:
Paparan polutan udara dapat menyebabkan berbagai masalah kesehatan, termasuk penyakit pernapasan, kardiovaskular, dan bahkan kematian dini. World Health Organization (WHO) telah menyatakan bahwa kualitas udara yang buruk merupakan salah satu ancaman kesehatan global.
2. Dampak Lingkungan:
Polutan seperti ozon dan nitrogen dioksida tidak hanya berdampak pada kesehatan manusia tetapi juga menyebabkan kerusakan lingkungan, seperti penurunan kualitas tanah dan air serta gangguan ekosistem.

# Business Understanding

## Problem Statements

Dengan meningkatnya polusi udara di daerah perkotaan, diperlukan sistem yang dapat memprediksi kualitas udara.

## Goals

Memprediksi nilai AQI (Air Quality Index) berdasarkan parameter polusi udara menggunakan machine learning.

## Solutions

1. Pemrosesan Data:
Melakukan normalisasi menggunakan metode Min-Max Scaling untuk memastikan semua parameter berada dalam skala yang sama.
Membagi data menjadi subset pelatihan dan validasi untuk menghindari overfitting.
2. Pemilihan Model:
Menggunakan model Neural Networks (CNN + LSTM) untuk menangkap pola temporal.
Memanfaatkan teknik dropout untuk menghindari overfitting.
3. Evaluasi Model:
Menggunakan Mean Absolute Error (MAE) sebagai metrik evaluasi utama.
Membandingkan prediksi model dengan data aktual melalui visualisasi.

# Datasets

Dataset yang digunakan berisi data kualitas udara harian Chicago-Naperville-Elgin, IL-IN-WI dari tahun 2015-2024 dengan fitur-fitur sebagai berikut:
- Overall AQI Value: Indeks kualitas udara yang mencerminkan tingkat risiko kesehatan.
- CO (Karbon Monoksida): Konsentrasi gas CO dalam udara.
- Ozon (O₃): Konsentrasi gas ozon.
- PM10: Partikel udara dengan diameter ≤ 10 µm.
- PM2.5: Partikel udara dengan diameter ≤ 2.5 µm.
- NO₂ (Nitrogen Dioksida): Konsentrasi gas nitrogen dioksida.

Sumber dataset : https://www.epa.gov/outdoor-air-quality-data/air-quality-index-daily-values-report

# Import Library
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Conv1D

"""# Load Data"""

df = pd.read_csv('aqidaily.csv', sep=';')

df

df['Date'] = pd.to_datetime(df['Date'])
df.set_index('Date', inplace=True)
df = df.drop(columns=['Main Pollutant','Site Name (of Overall AQI)','Site ID (of Overall AQI)','Source (of Overall AQI)'])
df.head()

"""# Exploratory Data Analysis (EDA)

## Info Data
"""

df.info()

df.describe()

"""## Checking Missing Value"""

# Mengecek missing values
print(df.isnull().sum())

"""## Checking Outliers"""

# Boxplot untuk setiap fitur
plt.figure(figsize=(10, 6))
sns.boxplot(data=df[['CO', 'Ozone', 'PM10', 'PM25', 'NO2', 'Overall AQI Value']])
plt.title('Outliers Check')
plt.show()

# Menyiapkan grid
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

# List kolom yang ingin divisualisasikan
columns = ['CO', 'Ozone', 'PM10', 'PM25', 'NO2', 'Overall AQI Value']

# Mengatur tampilan subplot
for i, col in enumerate(columns):
    ax = axes[i // 3, i % 3]
    sns.histplot(df[col], kde=True, ax=ax)
    ax.set_title(f'Distribution of {col}')

# Menyesuaikan layout
plt.tight_layout()
plt.show()

# Korelasi fitur dengan AQI
corr = df.corr()
sns.heatmap(corr, annot=True, cmap='coolwarm')
plt.title('Feature Correlation with Overall AQI Value')
plt.show()

"""Gambar di atas menunjukan Korelasi data. Korelasi data adalah ukuran yang menunjukkan sejauh mana dua variabel atau lebih berhubungan satu sama lain. Semakin kuat korelasi maka nilai koefisien akan mendekati 1 atau bernilai 1.

# Data Preprocessing

## Handling Missing Value
"""

df.interpolate(method='time', inplace=True)  # Interpolasi berdasarkan waktu

# Mengecek missing values
print(df.isnull().sum())

"""## Handling Outliers"""

df['PM25'] = np.clip(df['PM25'], df['PM25'].quantile(0.01), df['PM25'].quantile(0.99))
df['Overall AQI Value'] = np.clip(df['Overall AQI Value'], df['Overall AQI Value'].quantile(0.01), df['Overall AQI Value'].quantile(0.99))
df['Ozone'] = np.clip(df['Ozone'], df['Ozone'].quantile(0.01), df['Ozone'].quantile(0.99))
df['PM10'] = np.clip(df['PM10'], df['PM10'].quantile(0.01), df['PM10'].quantile(0.99))

# Boxplot untuk setiap fitur
plt.figure(figsize=(10, 6))
sns.boxplot(data=df[['CO', 'Ozone', 'PM10', 'PM25', 'NO2', 'Overall AQI Value']])
plt.title('Outliers Check')
plt.show()

"""## Normalisasi Data"""

def normalize_series(data, min, max):
    # Normalisasi data dengan formula (data - min) / (max - min)
    data = (data - min) / (max - min)
    return data

data =df.values

data_norm = normalize_series(data, data.min(axis=0), data.max(axis=0))
data_norm

"""## Split Data"""

split_time = int(len(data_norm) * 0.8)

x_train = data_norm[:split_time]
x_valid = data_norm[split_time:]

print('train : ', len(x_train))
print('validation : ', len(x_valid))

"""# Modeling"""

def windowed_dataset(series, batch_size, n_past=31, n_future=31, shift=1):
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(size=(n_past + n_future), shift = shift, drop_remainder = True)
    ds = ds.flat_map(lambda w: w.batch(n_past + n_future))
    ds = ds.shuffle(len(series))
    ds = ds.map(
        lambda w: (w[:n_past], w[n_past:])
    )
    return ds.batch(batch_size).prefetch(1)

N_FEATURES = len(df.columns)
BATCH_SIZE = 64
N_PAST = 7
N_FUTURE = 7
SHIFT = 1

train_set = windowed_dataset(series=x_train,
                             batch_size=BATCH_SIZE,
                             n_past=N_PAST,
                             n_future=N_FUTURE,
                             shift=SHIFT)

valid_set = windowed_dataset(series=x_valid,
                             batch_size=BATCH_SIZE,
                             n_past=N_PAST,
                             n_future=N_FUTURE,
                             shift=SHIFT)

# Membuat model
model = tf.keras.models.Sequential([
    Conv1D(filters=32,
            kernel_size=3,
            padding="causal",
            activation="relu",
            input_shape=[N_PAST, 6],
            ),
    Bidirectional(LSTM(32, return_sequences=True)),
    Dense(64, activation="relu"),
    Dropout(0.2),
    Dense(32, activation="relu"),
    Dropout(0.3),
    Dense(8, activation="relu"),
    Dropout(0.1),
    Dense(N_FEATURES)
])

model.summary()

# Compile model
optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)
model.compile(loss='mae', optimizer=optimizer, metrics=['mae'])

# ModelCheckpoint
cp_path = 'model/my_model.h5'
checkpoint = ModelCheckpoint(cp_path, save_weights_only=True, save_best_only=True, monitor='val_loss', verbose=1)

# Training
history = model.fit(train_set,
                validation_data=valid_set,
                epochs=100,
                callbacks=[checkpoint])

"""# Evaluations"""

model.evaluate(valid_set)

print('Training loss (MAE)   : ', history.history['mae'][-1])
print('Validation loss (MAE) : ', history.history['val_mae'][-1])

# Visualisasi evaluasi pelatihan
plt.figure(figsize=(12, 6))
plt.plot(history.history['mae'], label='Training Loss')
plt.plot(history.history['val_mae'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss (MAE)')
plt.title('Training and Validation Loss')
plt.legend()
plt.grid(True)
plt.show()